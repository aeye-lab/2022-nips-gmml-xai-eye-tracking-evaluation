--- modules.py	2022-09-02 16:58:38.000000000 +0200
+++ new.py	2022-11-20 19:21:35.410445607 +0100
@@ -4,260 +4,315 @@
 #
 # Author: Dillon Lohr (djl70@txstate.edu)
 # Property of Texas State University.
+# Modified by: Daniel Krakowczyk (daniel.krakowczyk@uni-potsdam.de)
 
-from pathlib import Path
-from typing import Optional, Tuple
 
-import pandas as pd
+from collections import OrderedDict
+from typing import List, Optional, Union
+
+import numpy as np
 import pytorch_lightning as pl
 import torch
-from pytorch_lightning.utilities.types import EPOCH_OUTPUT, STEP_OUTPUT
-from pytorch_metric_learning import losses, miners
-from pytorch_metric_learning.utils import accuracy_calculator
+import torchvision
+from pytorch_metric_learning import losses
+from pytorch_metric_learning import miners
+from torch import nn
+from torch import optim
+from torchmetrics import Accuracy
+
+from .base import Base
+
+
+def init_weights(modules_list: Union[nn.Module, List[nn.Module]]) -> None:
+    if not isinstance(modules_list, List):
+        modules_list = [modules_list]
+
+    for m in modules_list:
+        if isinstance(m, nn.Conv1d):
+            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
+        elif isinstance(m, nn.BatchNorm1d):
+            nn.init.constant_(m.weight, 1.0)
+            nn.init.constant_(m.bias, 0.0)
+        elif isinstance(m, nn.Linear):
+            nn.init.constant_(m.bias, 0.0)
 
-from .networks import Classifier, SimpleDenseNet
 
+class ConvBlock(nn.Module):
+    """BatchNorm1d + ReLU + Conv1d + optional dense connection """
 
-class EyeKnowYouToo(pl.LightningModule):
     def __init__(
         self,
-        n_classes: int,
-        embeddings_filename: str,
-        embeddings_dir: str = "./embeddings",
-        w_metric_loss: float = 1.0,
-        w_class_loss: float = 0.1,
-        compute_map_at_r: bool = False,
+        input_channels: int,
+        output_channels: int,
+        conv_block_id: int,
+        dilation: int = 1,
+        add_dense_connection: bool = True,
+        skip_bn_relu: bool = False,
+        kernel_size: int = 3,
     ):
         super().__init__()
+        self.add_dense_connection = add_dense_connection
 
-        self.embedder = SimpleDenseNet(depth=9, output_dim=128)
-        self.classifier = Classifier(self.embedder.output_dim, n_classes)
+        layers = []
 
-        self.w_metric_loss = w_metric_loss
-        self.metric_criterion = losses.MultiSimilarityLoss()
-        self.metric_miner = miners.MultiSimilarityMiner()
-
-        self.w_class_loss = w_class_loss
-        self.class_criterion = torch.nn.CrossEntropyLoss()
-
-        self.compute_map_at_r = compute_map_at_r
-        self.map_at_r_calculator = (
-            accuracy_calculator.AccuracyCalculator(
-                include=["mean_average_precision_at_r"],
-                avg_of_avgs=True,
-                k="max_bin_count",
-            )
-            if self.compute_map_at_r
-            else None
+        if not skip_bn_relu:
+            layers.extend([
+                (
+                    f'bn_conv_{conv_block_id}',
+                    nn.BatchNorm1d(input_channels),
+                ),
+                (
+                    f'act_conv_{conv_block_id}',
+                    nn.ReLU(inplace=False),
+                ),
+            ])
+
+        layers.append(
+            (
+                f'conv_{conv_block_id}',
+                nn.Conv1d(
+                    input_channels,
+                    output_channels,
+                    kernel_size=kernel_size,
+                    stride=1,
+                    padding='same',
+                    dilation=dilation,
+                    bias=False,
+                ),
+            ),
         )
+        self.block = nn.Sequential(OrderedDict([*layers]))
 
-        self.embeddings_path = Path(embeddings_dir) / embeddings_filename
 
-    def forward(self, x):
-        out = self.embedder(x)
-        return out
-
-    def training_step(
-        self, batch: Tuple[torch.Tensor, torch.LongTensor], batch_idx: int
-    ) -> STEP_OUTPUT:
-        assert torch.is_grad_enabled()
-        assert self.training
-
-        inputs, metadata = batch
-        embeddings = self.embedder(inputs)
-
-        labels = metadata[:, 0]
-        metric_loss = self.metric_step(embeddings, labels)
-        class_loss = self.class_step(embeddings, labels)
-        total_loss = metric_loss + class_loss
-
-        self.log("train_loss", total_loss)
-        self.log("train_metric_loss", metric_loss)
-        self.log("train_class_loss", class_loss)
-        return {"loss": total_loss}
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        out = self.block(x)
 
-    def validation_step(
-        self,
-        batch: Tuple[torch.Tensor, torch.LongTensor],
-        batch_idx: int,
-        dataloader_idx: int,
-    ) -> Optional[STEP_OUTPUT]:
-        assert not torch.is_grad_enabled()
-        assert not self.training
-
-        inputs, metadata = batch
-        embeddings = self.embedder(inputs)
-        return {"embeddings": embeddings, "metadata": metadata}
+        if self.add_dense_connection:
+            return torch.cat([x, out], dim=1)
+        else:
+            return out
+
+
+class ConvStack(nn.Module):
+    """Series of convolution blocks with optional dense connections."""
 
-    def test_step(
+    def __init__(
         self,
-        batch: Tuple[torch.Tensor, torch.LongTensor],
-        batch_idx: int,
-        dataloader_idx: Optional[int] = None,
-    ) -> Optional[STEP_OUTPUT]:
-        assert not torch.is_grad_enabled()
-        assert not self.training
-
-        inputs, metadata = batch
-        embeddings = self.embedder(inputs)
-        return {"embeddings": embeddings, "metadata": metadata}
-
-    def validation_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:
-        full_val_outputs = outputs[0]
-        sum_val_loss = 0
-        for batch_output in full_val_outputs:
-            embeddings = batch_output["embeddings"]
-            metadata = batch_output["metadata"]
-            labels = metadata[:, 0]
-            metric_loss = self.metric_step(embeddings, labels)
-            sum_val_loss += metric_loss
-        mean_val_loss = sum_val_loss / len(full_val_outputs)
-        self.log("val_metric_loss", mean_val_loss)
-
-        if not self.compute_map_at_r:
-            return
-
-        def process_batch_outputs(list_of_batch_outputs, split):
-            embeddings = [x["embeddings"] for x in list_of_batch_outputs]
-            metadata = [x["metadata"] for x in list_of_batch_outputs]
-            embeddings = torch.cat(embeddings, dim=0)
-            metadata = torch.cat(metadata, dim=0)
-
-            labels = metadata[:, 0]
-
-            result_dict = self.map_at_r_calculator.get_accuracy(
-                embeddings,
-                embeddings,
-                labels,
-                labels,
-                embeddings_come_from_same_source=True,
-            )
-            map_at_r = result_dict["mean_average_precision_at_r"]
-            self.log(split + "_map_at_r", map_at_r)
+        n_layers: int,
+        input_channels: int,
+        growth_rate: int,
+        max_dilation: int = 64,
+        add_dense_connections: bool = True,
+        skip_bn_relu_first_layer: bool = True,
+        kernel_size: int = 3,
+    ):
+        super().__init__()
 
-        val_tex_outputs = outputs[1]
-        process_batch_outputs(val_tex_outputs, "val")
+        dilation_exp_mod = int(np.log2(max_dilation)) + 1
 
-        train_tex_outputs = outputs[2]
-        process_batch_outputs(train_tex_outputs, "train")
+        def dilation_at_i(i: int) -> int:
+            return 2 ** (i % dilation_exp_mod)
 
-    def test_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:
-        def process_batch_outputs(list_of_batch_outputs, split):
-            embeddings = [x["embeddings"] for x in list_of_batch_outputs]
-            metadata = [x["metadata"] for x in list_of_batch_outputs]
-            embeddings = torch.cat(embeddings, dim=0).detach().cpu().numpy()
-            metadata = torch.cat(metadata, dim=0).detach().cpu().numpy()
-
-            embed_dim = embeddings.shape[1]
-            embedding_dict = {
-                f"embed_dim_{i:03d}": embeddings[:, i]
-                for i in range(embed_dim)
-            }
-            full_dict = {
-                "nb_round": metadata[:, 1],
-                "nb_subject": metadata[:, 2],
-                "nb_session": metadata[:, 3],
-                "nb_task": metadata[:, 4],
-                "nb_subsequence": metadata[:, 5],
-                "exclude": metadata[:, 6],
-                **embedding_dict,
-            }
-            df = pd.DataFrame(full_dict)
-            df = df.sort_values(
-                by=[
-                    "nb_round",
-                    "nb_subject",
-                    "nb_session",
-                    "nb_task",
-                    "nb_subsequence",
-                ],
-                axis=0,
-                ascending=True,
-            )
-            path = self.embeddings_path.with_name(
-                split + "_" + self.embeddings_path.name
+        if add_dense_connections:
+            # each conv layer increases input channels by additive growth rate
+            layer_input_channels = [
+                input_channels + i * growth_rate
+                for i in range(n_layers)
+            ]
+        else:
+            # first conv layer gets number of input channels
+            # all next layers only get static value of growth rate as input channels
+            layer_input_channels = [input_channels] + [
+                growth_rate for _ in range(n_layers - 1)
+            ]
+
+        conv_blocks = [
+            (
+                f'block_conv_{conv_block_id}',
+                ConvBlock(
+                    input_channels=layer_input_channels[conv_block_id],
+                    output_channels=growth_rate,
+                    dilation=dilation_at_i(conv_block_id),
+                    add_dense_connection=add_dense_connections,
+                    skip_bn_relu=(conv_block_id == 0 and skip_bn_relu_first_layer),
+                    kernel_size=kernel_size,
+                    conv_block_id=conv_block_id,
+                ),
             )
-            path.parent.mkdir(parents=True, exist_ok=True)
-            df.to_csv(path, index=False)
+            for conv_block_id in range(n_layers)
+        ]
+        self.stack = nn.Sequential(OrderedDict([*conv_blocks]))
 
-        if isinstance(outputs[0], list):
-            # More than one test dataloader was used, so this is
-            # GazeBase.  The first loader is the held-out test set, and
-            # the second loader is the full validation set.
-            test_outputs = outputs[0]
-            process_batch_outputs(test_outputs, "test")
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.stack(x)
+        return x
 
-            val_outputs = outputs[1]
-            process_batch_outputs(val_outputs, "val")
-        else:
-            # Only one test dataloader was used, so this is JuDo1000
-            process_batch_outputs(outputs, "judo")
 
-    def metric_step(
-        self, embeddings: torch.Tensor, labels: torch.LongTensor
-    ) -> torch.Tensor:
-        if self.w_metric_loss <= 0.0:
-            return 0.0
+class ClassifierBlock(nn.Module):
+    """Optional classification block"""
+
+    def __init__(self, input_dim: int, n_classes: int):
+        super().__init__()
+        self.input_dim = input_dim
+        self.n_classes = n_classes
+
+        self.bn = nn.BatchNorm1d(input_dim)
+        self.relu = nn.ReLU(inplace=False)
+        self.fc = nn.Linear(input_dim, n_classes)
+
+        init_weights(self.modules())
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.bn(x)
+        x = self.relu(x)
+        x = self.fc(x)
+        return x
+
+
+
+class EKY2(Base):
+    """
+    Network with a single dense block.
 
-        mined_indices = (
-            None
-            if self.metric_miner is None
-            else self.metric_miner(embeddings, labels)
+    References
+    ----------
+    https://github.com/andreasveit/densenet-pytorch/blob/master/densenet.py
+    """
+
+    def __init__(
+        self,
+        input_channels: int,
+        output_units: int,
+        embedding_dim: int = 128,
+        depth: int = 8,
+        growth_rate: int = 32,
+        max_dilation: int = 64,
+        kernel_size: int = 3,
+        add_dense_connections: bool = True,
+        loss_weights_categorical: float = 0.1,
+        loss_weights_metric: float = 1.0,
+        default_forward_mode: str = 'embedding',
+    ):
+        super().__init__()
+        self.save_hyperparameters()
+
+        self.categorical_loss_func = nn.CrossEntropyLoss()
+        self.metric_loss_miner = miners.MultiSimilarityMiner(epsilon=0.1)
+        self.metric_loss_func = losses.MultiSimilarityLoss(
+            alpha=2, beta=50, base=0.5,
         )
-        metric_loss = self.metric_criterion(embeddings, labels, mined_indices)
+        self.loss_weights = {
+            'categorical': loss_weights_categorical,
+            'metric': loss_weights_metric,
+        }
+
+        self.metrics = nn.ModuleDict({
+            'accuracy': Accuracy(),
+        })
+
+        self.default_forward_mode = default_forward_mode
+
+        n_fixed_layers = 1  # embedding layer
+        n_layers_per_block = depth - n_fixed_layers
+        assert n_layers_per_block > 0, "`depth` is too small"
+
+        # All conv blocks as a single stack
+        self.conv_stack = ConvStack(
+            n_layers_per_block,
+            input_channels,
+            growth_rate,
+            max_dilation=max_dilation,
+            add_dense_connections=add_dense_connections,
+            skip_bn_relu_first_layer=True,
+            kernel_size=kernel_size,
+        )
+
+        if add_dense_connections:
+            # each conv layer increases input channels by additive growth rate
+            conv_stack_output_channels = input_channels + n_layers_per_block * growth_rate
+        else:
+            # first conv layer gets number of input channels
+            # all next layers only get static value of growth rate as input channels
+            conv_stack_output_channels = growth_rate
+
+        # Global average pooling and embedding layer
+        self.global_pooling = nn.Sequential(OrderedDict([
+            ('global_bn', nn.BatchNorm1d(conv_stack_output_channels)),
+            ('global_relu', nn.ReLU(inplace=False)),
+            ('global_pool', nn.AdaptiveAvgPool1d(1)),
+            ('flatten', nn.Flatten()),
+        ]))
 
-        weighted_metric_loss = metric_loss * self.w_metric_loss
-        return weighted_metric_loss
+        self.fc_embed = nn.Linear(conv_stack_output_channels, embedding_dim)
+        self.fc_class = ClassifierBlock(embedding_dim, output_units)
 
-    def class_step(
-        self, embeddings: torch.Tensor, labels: torch.LongTensor
+        # Initialize weights
+        init_weights(self.modules())
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        mode: Optional[str] = None,
     ) -> torch.Tensor:
-        # Since we have class-disjoint datasets, only compute class loss
-        # on the training set.  We know we're working with the train set
-        # if `self.embedder.training` is True.
-        if (
-            self.classifier is None
-            or self.w_class_loss <= 0.0
-            or not self.training
-        ):
-            return 0.0
-
-        # When logits and labels are on the GPU, we get an error on the
-        # backward pass.  For some reason, transferring them to the CPU
-        # fixes the error.
-        #
-        # Full error below (with several instances of this error for
-        # different threads, e.g., [6,0,0], [7,0,0], and [13,0,0]):
-        # .../pytorch_1634272168290/work/aten/src/ATen/native/cuda/Loss.cu:455:
-        # nll_loss_backward_reduce_cuda_kernel_2d: block: [0,0,0],
-        # thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
-        logits = self.classifier(embeddings)
-        class_loss = self.class_criterion(logits.cpu(), labels.cpu())
+        x = self.conv_stack(x)
+        x = self.global_pooling(x)
+
+        if mode is None:
+            mode = self.default_forward_mode
 
-        weighted_class_loss = class_loss * self.w_class_loss
-        return weighted_class_loss
+        if mode == 'class':
+            x = self.fc_embed(x)
+            return self.fc_class(x)
+        elif mode == 'embedding':
+            return self.fc_embed(x)
+        elif mode == 'both':
+            embedding = self.fc_embed(x)
+            class_out = self.fc_class(embedding)
+            return class_out, embedding
+        else:
+            raise ValueError(f'unsupported mode `{mode}`')
 
     def configure_optimizers(self):
-        opt = torch.optim.Adam(self.parameters())
-        sched = torch.optim.lr_scheduler.OneCycleLR(
-            optimizer=opt,
+        optimizer = optim.Adam(
+            params=self.parameters(),
+            lr=0.0001,
+            betas=(0.9, 0.999),
+            weight_decay=0.0001,
+        )
+        scheduler = torch.optim.lr_scheduler.OneCycleLR(
+            optimizer,
             max_lr=0.01,
-            epochs=100,
             steps_per_epoch=1,
-            cycle_momentum=False,
-            div_factor=100.0,
-            final_div_factor=1000.0,
+            epochs=100,
         )
-        return {
-            "optimizer": opt,
-            "lr_scheduler": {"scheduler": sched, "interval": "epoch"},
+        return [optimizer], [scheduler]
+
+    def any_step(self, batch, batch_idx, set_prefix: str):
+        xb, yb = batch
+        labels = torch.argmax(yb, dim=1)
+        yb_pred, embeddings = self(xb, mode='both')
+
+        miner_output = self.metric_loss_miner(embeddings, labels)
+        metric_loss = self.metric_loss_func(embeddings, labels, miner_output)
+        weighted_metric_loss = metric_loss * self.loss_weights['metric']
+
+        cat_loss = self.categorical_loss_func(yb_pred, yb)
+        weighted_cat_loss = cat_loss * self.loss_weights['categorical']
+
+        loss = weighted_metric_loss + weighted_cat_loss
+
+        outputs = {
+            f'{set_prefix}_loss': loss.detach(),
+            f'{set_prefix}_metric_loss': metric_loss.detach(),
+            f'{set_prefix}_categorical_loss': cat_loss.detach(),
         }
 
-    def optimizer_zero_grad(
-        self,
-        epoch: int,
-        batch_idx: int,
-        optimizer: torch.optim.Optimizer,
-        optimizer_idx: int,
-    ) -> None:
-        optimizer.zero_grad(set_to_none=True)
+        if set_prefix == 'train':
+            outputs['loss'] = loss
+
+        yb_uncat = torch.argmax(yb.squeeze(), dim=1)
+        for metric_name, metric_func in self.metrics.items():
+            metric_value = metric_func(yb_pred, yb_uncat).detach()
+            outputs[f'{set_prefix}_{metric_name}'] = metric_value
+
+        return outputs
